{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Snowflake using `Spark (Scala & PySpark)`\n",
    "\n",
    "\n",
    "#### Topics covered in this example\n",
    "* Installing Snowflake connector for Spark\n",
    "* Connecting to Snowflake using `PySpark`\n",
    "* Connecting to Snowflake using `Spark Scala`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Introduction](#Introduction)\n",
    "3. [Install dependency libraries](#Install-dependency-libraries)\n",
    "4. [Connect to Snowflake using `Spark - PySpark`](#Connect-to-Snowflake-using-Spark---PySpark)\n",
    "   * [Read Data from Snowflake table](#Read-data-from-Snowflake-table-using-Spark---PySpark)\n",
    "   * [Write Data to Snowflake table](#Write-data-to-Snowflake-table-using-Spark---PySpark)\n",
    "5. [Connect to Snowflake using `Spark - Scala`](#Connect-to-Snowflake-using-Spark---Scala)\n",
    "   * [Read Data from Snowflake table](#Read-data-from-Snowflake-table-using-Spark---Scala)\n",
    "   * [Write Data to Snowflake table](#Write-data-to-Snowflake-table-using-Spark---Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Prerequisites\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE :</b> In order to execute this notebook successfully as is, please ensure the following prerequisites are completed.</div>\n",
    "\n",
    "* The Amazon EMR cluster attached to this notebook should have the Spark application installed.\n",
    "* This notebook is tested with Amazon EMR 6.4.0\n",
    "* This is a multi-language notebook for EMR Studio which is supported from Amazon EMR 6.4.0 and later\n",
    "* This example downloads the Snowflake connector from Maven, hence the EMR cluster attached to this notebook must have internet connectivity.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example we use `Spark (Scala & PySpark)` to connect to a table in Snowflake using the Snowflake connector for Spark.\n",
    "\n",
    "The Snowflake Connector for Spark (“Spark connector”) brings Snowflake into the Apache Spark ecosystem, enabling Spark to read data from, and write data to, Snowflake.\n",
    "\n",
    "Snowflake supports three versions of Spark: Spark 2.4, Spark 3.0, and Spark 3.1. There is a separate version of the Snowflake connector for each version of Spark. Use the correct version of the connector for your version of Spark.\n",
    "\n",
    "The connector runs as a Spark plugin and is provided as a Spark package (spark-snowflake).\n",
    "\n",
    "For more information please find the [documentation here](#https://docs.snowflake.com/en/user-guide/spark-connector.html)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependency libraries\n",
    "\n",
    "- We will need the following three libraries for Snowflake connector for Spark to work:\n",
    "\n",
    "  * `spark-snowflake.jar`\n",
    "  * `snowflake-jdbc.jar`\n",
    "  * `snowflake-ingest-sdk.jar`\n",
    "\n",
    "* We will download these from [Maven Central Repository](https://search.maven.org/classic/#search%7Cga%7C1%7Cg%3A%22net.snowflake%22). You can manually download these and place it on a distributed file system like Amazon S3 or HDFS.\n",
    "\n",
    "* For this example, we will download these on the Amazon EMR master node and then place these files on HDFS. We will use `hdfs:///tmp/spark-snowflake/lib/` to host these files on HDFS. Feel free to change this location to something which is appropriate for your requirements.\n",
    "\n",
    "* Execute the below cell or you can manually execute these commands on the EMR master node to get the jar libraries and place them on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Create directory to download the jar files\n",
    "mkdir -p /tmp/spark-snowflake-jars/\n",
    "\n",
    "# Download jar files\n",
    "wget -O /tmp/spark-snowflake-jars/spark-snowflake.jar https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.2-spark_3.1/spark-snowflake_2.12-2.9.2-spark_3.1.jar\n",
    "wget -O /tmp/spark-snowflake-jars/snowflake-jdbc.jar https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.10/snowflake-jdbc-3.13.10.jar\n",
    "wget -O /tmp/spark-snowflake-jars/snowflake-ingest-sdk.jar https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3.jar\n",
    "\n",
    "# Place the library jar files on HDFS\n",
    "export JAVA_HOME='/etc/alternatives/jre'\n",
    "hdfs dfs -mkdir -p /tmp/spark-snowflake/lib/\n",
    "hdfs dfs -copyFromLocal -f /tmp/spark-snowflake-jars/*.jar /tmp/spark-snowflake/lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect to Snowflake using `Spark - PySpark`\n",
    "\n",
    "We start off by setting the libraries jar for the Spark session. We will do this by running the following cell\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>NOTE :</b> Please note that the below cell will be common for <u><i>PySpark</i></u> and <u><i>Scala</i></u>. If using specifically for either of these kernels, make sure you execute the `configure` cell below.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\": \"hdfs:///tmp/spark-snowflake/lib/spark-snowflake.jar,hdfs:///tmp/spark-snowflake/lib/snowflake-jdbc.jar,hdfs:///tmp/spark-snowflake/lib/snowflake-ingest-sdk.jar\"        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Snowflake table using `Spark - PySpark`\n",
    "\n",
    "We start off with an example of reading the data from a table in Snowflake using `Spark - PySpark`\n",
    "\n",
    "In this example, we will connect to Snowflake with the account idenfier `abc12345` with the user `SNOWFLAKE_USER` and password `My_Password`. The table we will query is `CUSTOMER` which is present within schema `TPCH_SF1` inside the database `SNOWFLAKE_SAMPLE_DATA`.\n",
    "\n",
    "Please make sure you replace these values with the ones appropriate for your environment/setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://abc12345.us-east-1.snowflakecomputing.com\",\n",
    "  \"sfUser\" : \"SNOWFLAKE_USER\",\n",
    "  \"sfPassword\" : \"My_Password\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCH_SF1\"\n",
    "}\n",
    "\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "dfPySpark = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "  .options(**sfOptions) \\\n",
    "  .option(\"query\", \"SELECT * FROM CUSTOMER\") \\\n",
    "  .load()\n",
    "\n",
    "dfPySpark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to Snowflake table using `Spark - PySpark`\n",
    "\n",
    "This section describes how we can write data to a table in Snowflake using `Spark - PySpark`\n",
    "\n",
    "We first create a sample data frame `sampleDf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampleDf = spark.createDataFrame([\n",
    " (\"1\", \"john jones\"),\n",
    " (\"2\", \"tracey smith\"),\n",
    " (\"3\", \"amy sander\")\n",
    "],[\"id\", \"name\"])\n",
    "\n",
    "sampleDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T21:25:18.196176Z",
     "iopub.status.busy": "2022-01-22T21:25:18.195891Z",
     "iopub.status.idle": "2022-01-22T21:25:20.339540Z",
     "shell.execute_reply": "2022-01-22T21:25:20.338732Z",
     "shell.execute_reply.started": "2022-01-22T21:25:18.196150Z"
    },
    "tags": []
   },
   "source": [
    "We will then write the data frame `samepleDf` to Snowflake with the account idenfier `abc12345` with the user `SNOWFLAKE_USER` and password `My_Password`. The table we will write to is `SAMPLETABLE` which is present within schema `PUBLIC` inside the database `SAMPLEDB`.\n",
    "\n",
    "Please make sure you replace these values with the ones appropriate for your environment/setup.\n",
    "\n",
    "Alternatively, you can use `query` instead of `dbtable` to provide specific SQL statement to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://abc12345.us-east-1.snowflakecomputing.com\",\n",
    "  \"sfUser\" : \"SNOWFLAKE_USER\",\n",
    "  \"sfPassword\" : \"My_Password\",\n",
    "  \"sfDatabase\" : \"SAMPLEDB\",\n",
    "  \"sfSchema\" : \"PUBLIC\"\n",
    "}\n",
    "\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "sampleDf.write \\\n",
    "    .format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"SAMPLETABLE\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Snowflake using `Spark - Scala`\n",
    "\n",
    "We We start off by setting the libraries jar for the Spark session. We will do this by running the following cell\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>NOTE :</b> Please note that the below cell will be common for <u><i>PySpark</i></u> and <u><i>Scala</i></u>. If using specifically for either of these kernels, make sure you execute the `configure` cell below.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\": \"hdfs:///tmp/spark-snowflake/lib/spark-snowflake.jar,hdfs:///tmp/spark-snowflake/lib/snowflake-jdbc.jar,hdfs:///tmp/spark-snowflake/lib/snowflake-ingest-sdk.jar\"        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Snowflake table using `Spark - Scala`\n",
    "\n",
    "We start with an example of reading the data from a table in Snowflake using `Spark - Scala`\n",
    "\n",
    "In this example, we will connect to Snowflake with the account idenfier `abc12345` with the user `SNOWFLAKE_USER` and password `My_Password`. The table we will query is `CUSTOMER` which is present within schema `TPCH_SF1` inside the database `SNOWFLAKE_SAMPLE_DATA`.\n",
    "\n",
    "Please make sure you replace these values with the ones appropriate for your environment/setup.\n",
    "\n",
    "Alternatively, you can use `query` instead of `dbtable` to provide specific SQL statement to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%scalaspark\n",
    "\n",
    "val SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "var sfOptionsScala = Map(\n",
    "    \"sfURL\" -> \"https://abc12345.us-east-1.snowflakecomputing.com/\",\n",
    "    \"sfUser\" -> \"SNOWFLAKE_USER\",\n",
    "    \"sfPassword\" -> \"My_Password\",\n",
    "    \"sfDatabase\" -> \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "    \"sfSchema\" -> \"TPCH_SF1\",\n",
    "  )\n",
    "\n",
    "val dfScala = spark.read.\n",
    "    format(SNOWFLAKE_SOURCE_NAME).\n",
    "    options(sfOptionsScala).\n",
    "    option(\"dbtable\", \"CUSTOMER\").\n",
    "    load()\n",
    "\n",
    "dfScala.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to Snowflake table using `Spark - Scala`\n",
    "\n",
    "This section describes how we can write data to a table in Snowflake using `Spark - Scala`\n",
    "\n",
    "We first create a sample data frame `sampleDfScala`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%scalaspark\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val columns = Seq(\"id\", \"name\")\n",
    "\n",
    "val data = Seq((\"1\", \"john jones\"),\n",
    "               (\"2\", \"tracey smith\"),\n",
    "               (\"3\", \"amy sanders\"))\n",
    "\n",
    "val sampleDfScala = data.toDF(columns:_*)\n",
    "\n",
    "sampleDfScala.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then write the data frame `samepleDfScala` to Snowflake with the account idenfier `abc12345` with the user `SNOWFLAKE_USER` and password `My_Password`. The table we will write to is `SAMPLETABLESCALA` which is present within schema `PUBLIC` inside the database `SAMPLEDB`.\n",
    "\n",
    "Please make sure you replace these values with the ones appropriate for your environment/setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%scalaspark\n",
    "\n",
    "val SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "var sfOptionsScala = Map(\n",
    "    \"sfURL\" -> \"https://abc12345.us-east-1.snowflakecomputing.com/\",\n",
    "    \"sfUser\" -> \"SNOWFLAKE_USER\",\n",
    "    \"sfPassword\" -> \"My_Password\",\n",
    "    \"sfDatabase\" -> \"SAMPLEDB\",\n",
    "    \"sfSchema\" -> \"PUBLIC\",\n",
    "  )\n",
    "\n",
    "sampleDfScala.write.\n",
    "    format(SNOWFLAKE_SOURCE_NAME).\n",
    "    options(sfOptionsScala).\n",
    "    option(\"dbtable\", \"SAMPLETABLESCALA\").\n",
    "    mode(\"Overwrite\").\n",
    "    save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
